#wiley

Data engineering encompasses a broad set of procedures, tools, and skill sets that govern and facilitate the flow of data. Data engineers are experts at making data ready for consumption by working with multiple systems and tools.
#data 

Data engineers make an organization’s data “production ready” by putting it into a usable form, and typically in a centralized repository or cloud data platform.

Data engineers are focused primarily on building and maintaining data pipelines that transport data through different steps and put it into a usable state. Their charter is to ensure the data is clean, reliable, and properly transformed for whatever the user community requires, including running queries for front-line workers, generating reports for management, or training machine learning models for data scientists. They are skilled at wrangling data from hundreds of sources, including enterprise applications, software-as-a-service (SaaS) solutions, and weblog data, that generate immense amounts of data in unique formats.

The data engineering process encompasses the overall effort required to create data pipelines that automate the transfer of data from place to place and transform that data into a specific format for a certain type of analysis (see Figure 1-1). In that sense, data engineering isn’t something you do once. It’s an ongoing practice that involves collecting, preparing, transforming, and delivering data. A data pipeline helps automate these tasks so they can be reliably repeated. It’s a practice more than a specific technology.
#transformation 

Extract, transform, and load (ETL), or what we now call data engineering, used to be much simpler. There was much less data in the world, of fewer types, and needed at a much slower pace. Enterprise data was moved from one system to another, and software professionals generally transmitted it in batch mode, as a bulk data load operation. When data needed to be shared, it was often moved through File Transfer Protocol (FTP), application programming interfaces (APIs), and web services.
#evolution #humans-and-machines 

ETL procedures orchestrated the movement of data and converted it into a common format.

Previously, businesses had to be selective about which data they collected and stored. Now, advanced data repositories such as data lakes, data warehouses, and cloud data platforms allow businesses to capture the data first and ask questions later.
#problemsolving 

This shift from seeking answers to specific questions to first exploring data in a much broader sense, a process known as data discovery, uncovers new realms of inquiry.
#discovery #pursuit-of-knowledge #data-information-knowledge #exploration

With streaming data, there is no start or end to the data: It simply comes in as a stream. You don’t always know when it will be coming, but you know it needs to be captured.

 It’s much simpler to manage your data when you can consolidate it all in one location as a single source of truth.

Maintaining the data in a raw (or less processed) state allows data scientists to keep their options open. This approach is quicker, because it leverages the power of modern data processing engines and cuts down on unnecessary data movement.

Whatever industry or market you operate in, having all your data on hand and readily accessible opens doors to new opportunities. This is especially true when that data is stored and managed in a consistent way, and when you can use the near-infinite capacity of the cloud to scale your data initiatives.
#opportunity #growth 

Think of it as the “plug in the wall” concept: You don’t worry about where your electricity comes from. You just plug in your appliances and they work. In this fashion, data integration tools and data orchestration tools “abstract out” the connection details and data access details to make many types of data accessible in a consistent way. 
#abstraction 

Analytical initiatives will succeed only if the right data is delivered at the right time, in the correct form, which lies mainly with the data preparation processes. This is especially true for advanced analytics such as machine learning.
#preparation 

Good data engineering requires a product mindset, considering data as the product, as much as it requires a specific set of tools and technologies. In recent years, principles from the DevOps world, initially created to encourage agile software development, have been applied to data modeling and design. A newer term, DataOps, is used to describe the practices that automate the data engineering cycle.
#product #agile 

Good DataOps procedures enable businesses to ensure data quality, properly manage versions of data, enforce data privacy regulations, and keep data applications moving through a continuous cycle of development, integration, testing, and production.

DataOps needs to start with data governance as the foundation. 

The goal of data transformation is to integrate, cleanse, augment, and model the data so the business can derive more value from it. Transformation processes ensure the data is accurate, easy to consume, and governed by consistent business rules and processes. You must determine what business logic you will need to incorporate and how you want to transform or structure the data to uphold the business rules. Ask yourself what you are trying to build, and why, such as what business processes you want to model or create.
#business #capitalism #value-and-worth #value 

Whenever possible, use ELT (instead of ETL) processes to push resource-hungry transformation work to your destination platform.

Data is fundamental to the workings of the enterprise. But if you want to push your data to consumers fast with limited resources, you need to simplify your data architecture. Get out of the business of managing infrastructure. Help the business leaders at your organization understand a basic fact: The quickest way to deliver analytic value is to stop managing hardware and software.
#simplicity 

data engineering is a team sport.
#teamwork #collaboration 

